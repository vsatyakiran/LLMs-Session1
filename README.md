# Agenda of this Journey:
## Session1: Intro NLP
* word2vec
* cbow
* skip-gram
* Glove
## Session2: NLP using DeepLearning
* GRU
* RNN
* Types of RNNs
* LSTMs 
* Practical

# Session3: Transformers
* Transformers
* Types of transformers
* Transformer Architecture

# Session4: Project on LLMs

# LLMs-Session1
Intro Natural Language Processing(NLP)

In session1 I have completed the intro to NLP

**Collab link** : https://colab.research.google.com/drive/1VFpHHxvLGkAIS-M9mD-GyRRkNaZpx98s?usp=sharing

**Twitter Sentiment Analysis dataset link** : https://www.kaggle.com/datasets/jp797498e/twitter-entity-sentiment-analysis

**About gensim library and Word2vecc Prebulit models** : https://github.com/piskvorky/gensim-data

**Explore visualization with tensorflow projector** : https://projector.tensorflow.org/

I have provided you vectors.tsv file, you can use that file for visualization with tensorflow projector


# LLMs-Session-3
Brief explanation of archtecture of Transformers(Encoder, decoder).

classification of transformers based on encoders and decoders.
* Encoder Only Models - BERT
* Decoder Only Models - GPT
* Encoder-Decoder Models - Seq2Seq
  
I have completed how self-attention machanism made revolution in NLP.

I have provided the PPT of the session above , go through it to get overview about Transformers.[Self Attention:](https://arxiv.org/abs/1706.03762)

Resources your need to explore to learn LLMs

Link : https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452

Link : https://www.linkedin.com/pulse/demystifying-multihead-attention-transformer-neural-network-taneja/?trackingId=o6REbeohSTSI8%2BqOLlJSTQ%3D%3D

Link : https://towardsdatascience.com/foundations-of-nlp-explained-bleu-score-and-wer-metrics-1a5ba06d812b

[Self Attention:](https://arxiv.org/abs/1706.03762)





